# üìö Papers Fundamentales de la IA Moderna

Una colecci√≥n curada de los papers m√°s influyentes que han definido el panorama de la Inteligencia Artificial moderna. Ideal para tener una referencia r√°pida de los pilares te√≥ricos del Deep Learning.

---

## ü§ñ Arquitecturas Transformer y Modelos de Lenguaje (LLMs)

### Attention Is All You Need
- **Autores:** Ashish Vaswani, et al.
- **A√±o:** 2017
- **Publicaci√≥n:** [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)
- **Resumen R√°pido:** Introduce la arquitectura **Transformer**, basada √∫nicamente en mecanismos de atenci√≥n. Elimin√≥ la necesidad de recurrencia (RNNs) y permiti√≥ una paralelizaci√≥n masiva, sentando las bases para la mayor√≠a de los modelos de lenguaje modernos como GPT y BERT.

---

### BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- **Autores:** Jacob Devlin, et al.
- **A√±o:** 2018
- **Publicaci√≥n:** [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)
- **Resumen R√°pido:** Propuso un m√©todo de pre-entrenamiento bidireccional (**BERT**) que permiti√≥ a los modelos entender el contexto de una palabra a partir de su entorno izquierdo y derecho. Revolucion√≥ las tareas de Comprensi√≥n de Lenguaje Natural (NLU).

---

### Language Models are Unsupervised Multitask Learners (GPT-2)
- **Autores:** Alec Radford, Jeffrey Wu, et al.
- **A√±o:** 2019
- **Publicaci√≥n:** [Ver Paper (OpenAI)](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models-are-unsupervised-multitask-learners.pdf)
- **Resumen R√°pido:** Demostr√≥ la capacidad de los modelos de lenguaje a gran escala (**GPT-2**) para realizar una sorprendente variedad de tareas sin entrenamiento espec√≠fico (fine-tuning), mostrando un fuerte potencial de aprendizaje *zero-shot*.

---

### Language Models are Few-Shot Learners (GPT-3)
- **Autores:** Tom B. Brown, et al.
- **A√±o:** 2020
- **Publicaci√≥n:** [arXiv:2005.14165](https://arxiv.org/abs/2005.14165)
- **Resumen R√°pido:** Llev√≥ los LLMs a una escala sin precedentes (175 mil millones de par√°metros). Introdujo el concepto de "in-context learning" o aprendizaje *few-shot*, donde el modelo puede aprender a realizar una tarea con solo ver unos pocos ejemplos en el prompt.

---

## üëÅÔ∏è Visi√≥n por Computador y Modelos Generativos

### ImageNet Classification with Deep Convolutional Neural Networks (AlexNet)
- **Autores:** Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton
- **A√±o:** 2012
- **Publicaci√≥n:** [Ver Paper (NIPS)](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
- **Resumen R√°pido:** Este paper (**AlexNet**) gan√≥ el desaf√≠o ImageNet de 2012 por un margen enorme, demostrando la superioridad de las Redes Neuronales Convolucionales (CNNs) profundas y dando inicio a la revoluci√≥n del Deep Learning.

---

### Deep Residual Learning for Image Recognition (ResNet)
- **Autores:** Kaiming He, et al.
- **A√±o:** 2015
- **Publicaci√≥n:** [arXiv:1512.03385](https://arxiv.org/abs/1512.03385)
- **Resumen R√°pido:** Introdujo las "conexiones residuales" (*skip connections*), que permitieron entrenar redes neuronales mucho m√°s profundas (m√°s de 150 capas) sin sufrir el problema del gradiente desvaneciente. **ResNet** es una arquitectura fundamental en visi√≥n por computador.

---

### Generative Adversarial Nets (GANs)
- **Autores:** Ian J. Goodfellow, et al.
- **A√±o:** 2014
- **Publicaci√≥n:** [arXiv:1406.2661](https://arxiv.org/abs/1406.2661)
- **Resumen R√°pido:** Present√≥ un marco de trabajo para entrenar modelos generativos donde dos redes (un **Generador** y un **Discriminador**) compiten entre s√≠. Esta arquitectura **GAN** revolucion√≥ la generaci√≥n de im√°genes, audio y otros datos realistas.

---

## ‚öôÔ∏è Optimizaci√≥n

### Adam: A Method for Stochastic Optimization
- **Autores:** Diederik P. Kingma, Jimmy Ba
- **A√±o:** 2014
- **Publicaci√≥n:** [arXiv:1412.6980](https://arxiv.org/abs/1412.6980)
- **Resumen R√°pido:** Propone el optimizador **Adam**, que adapta la tasa de aprendizaje para cada par√°metro combinando las ventajas de otros m√©todos como AdaGrad y RMSProp. Se convirti√≥ r√°pidamente en el optimizador por defecto para la mayor√≠a de las aplicaciones de Deep Learning.

---

## üéÆ Aprendizaje por Refuerzo (Reinforcement Learning)

### Playing Atari with Deep Reinforcement Learning (DQN)
- **Autores:** Volodymyr Mnih, et al.
- **A√±o:** 2013
- **Publicaci√≥n:** [arXiv:1312.5602](https://arxiv.org/abs/1312.5602)
- **Resumen R√°pido:** Fue el primer paper en combinar exitosamente el Aprendizaje por Refuerzo con una red neuronal profunda (CNN) para crear un agente general (**DQN**) que aprendi√≥ a jugar videojuegos de Atari a nivel sobrehumano, recibiendo √∫nicamente los p√≠xeles de la pantalla como entrada.
