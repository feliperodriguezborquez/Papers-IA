---
layout: default
lang: es
---
#  Papers Fundamentales de la IA Moderna

Una colecci贸n curada de los papers m谩s influyentes que han definido el panorama de la Inteligencia Artificial moderna. Ideal para tener una referencia r谩pida de los pilares te贸ricos del Deep Learning.

---

<div class="papers-list">

  <section class="paper-category">
    <h2> Arquitecturas Transformer y Modelos de Lenguaje (LLMs)</h2>

    <div class="paper-item" id="attention-is-all-you-need" data-category="LLMs, Transformers">
      <h3><a href="papers/attention-is-all-you-need.html">Attention Is All You Need</a></h3>
      <p class="paper-meta">
        <strong>Autores:</strong> Ashish Vaswani, et al.<br>
        <strong>A帽o:</strong> 2017<br>
        <strong>Publicaci贸n:</strong> <a href="https://arxiv.org/pdf/1706.03762">arXiv:1706.03762</a>
      </p>
      <p class="paper-summary">Introdujo la arquitectura **Transformer** (encoder-decoder), ocupando como 煤nica base el mecanismo de atenci贸n, prescindiendo as铆 de las antiguas RNNs. Adem谩s, realiz贸 toda la ingenier铆a necesaria para que el modelo sea funcional. Sent贸 las bases para los modernos LLMs.</p>
    </div>

    <div class="paper-item" id="gpt3" data-category="LLMs, Few-Shot Learning">
      <h3>Language Models are Few-Shot Learners (GPT-3)</h3>
      <p class="paper-meta">
        <strong>Autores:</strong> Tom B. Brown, et al.<br>
        <strong>A帽o:</strong> 2020<br>
        <strong>Publicaci贸n:</strong> <a href="https://arxiv.org/pdf/2005.14165">arXiv:2005.14165</a>
      </p>
      <p class="paper-summary">Llev贸 los LLMs a una escala sin precedentes (175 mil millones de par谩metros). Introdujo el concepto de "in-context learning" o aprendizaje *few-shot*, donde el modelo puede aprender a realizar una tarea con solo ver unos pocos ejemplos en el prompt.</p>
    </div>

    <div class="paper-item" id="next-token-prediction" data-category="LLMs, Conceptos Fundamentales">
      <h3><a href="next-token-prediction.html">Predicci贸n del Siguiente Token</a></h3>
      <p class="paper-meta">
        <strong>Tipo:</strong> Art铆culo Explicativo<br>
        <strong>Concepto Clave:</strong> Generaci贸n Autorregresiva
      </p>
      <p class="paper-summary">Explica de forma sencilla el motor de todos los LLMs modernos: el bucle de predecir la siguiente palabra (o token), a帽adirla al texto, y repetir. La base de la IA generativa actual.</p>
    </div>

  </section>

</div>
