---
layout: default
lang: es
title: Predicci√≥n del Siguiente Token
---
<div class="paper-detail">
  <div class="post-header">
    <a href="./">&larr; Volver a la lista</a>
    {% if other_lang_page %}
      <a href="{{ other_lang_url | relative_url }}" class="language-switch-button">{{ other_lang_text }}</a>
    {% endif %}
  </div>

  <h1>{{ page.title }}</h1>
  <p><em>IA en 5 min</em></p>

  <p>Toda la magia que ves en ChatGPT, Claude o Llama se sostiene sobre una sola idea. Una idea sorprendentemente simple que, de hecho, usas todos los d√≠as.</p>
  <p>Hoy rompemos el motor por dentro.</p>

  <hr>

  <h2>TL;DR</h2>
  <ul>
    <li><strong>El Juego:</strong> Un LLM es un "autocompletador" con esteroides. Su √∫nico trabajo es adivinar la palabra (o <span class="highlight-term definition" data-term-id="token">"token"</span>) m√°s probable que viene despu√©s de un texto.</li>
    <li><strong>El Bucle:</strong> Para escribir p√°rrafos, repite este juego una y otra vez: predice una palabra, la a√±ade al texto, y predice la siguiente.</li>
    <li><strong>El Resultado:</strong> La "inteligencia" emerge de la escala masiva de este simple proceso.</li>
  </ul>

  <hr>

  <h2>1. El Autocompletador M√°gico</h2>

  <p>Cuando tu tel√©fono te sugiere "tarde" despu√©s de que escribes "Nos vemos a la...", est√° haciendo una predicci√≥n b√°sica.</p>
  <p>Un LLM hace <em>exactamente lo mismo</em>, pero en lugar de haber sido entrenado con tus mensajes, ha sido entrenado con (casi) todo internet.</p>

  <p>Dale al modelo la frase:
  <code>"El gato se sent√≥ sobre la..."</code></p>

  <p>El modelo no "piensa" en un gato. Simplemente ha procesado billones de frases y sabe estad√≠sticamente qu√© suele venir despu√©s.</p>

  <blockquote>
    <p><strong>Mejora:</strong> He a√±adido un punto de fricci√≥n clave: la diferencia entre "palabra" y "token". Es esencial para la "claridad mental" que buscamos.</p>
  </blockquote>

  <h3>Un detalle clave: "Token" no es igual a "Palabra"</h3>

  <p>R√°pidamente: los modelos no ven "palabras", ven <span class="highlight-term definition" data-term-id="token">"tokens"</span>. Piensa en un <span class="highlight-term definition" data-term-id="token">token</span> como una s√≠laba o un trozo de palabra.</p>

  <ul>
    <li>La palabra <code>gato</code> es 1 <span class="highlight-term definition" data-term-id="token">token</span>.</li>
    <li>La palabra <code>corriendo</code> pueden ser 2 <span class="highlight-term definition" data-term-id="token">tokens</span>: <code>corr</code> + <code>iendo</code>.</li>
    <li>La palabra <code>Autopista</code> pueden ser 2 <span class="highlight-term definition" data-term-id="token">tokens</span>: <code>Auto</code> + <code>pista</code>.</li>
  </ul>

  <p>Esto les da flexibilidad. Cuando decimos "predecir la siguiente palabra", t√©cnicamente es "predecir el siguiente <span class="highlight-term definition" data-term-id="token">token</span>". <strong>Es un matiz t√©cnico, pero la idea del autocompletador sigue intacta.</strong></p>

  <h2>2. ¬øC√≥mo Elige la Palabra? (La Distribuci√≥n)</h2>

  <p>Aqu√≠ est√° el n√∫cleo. El modelo no te da <em>una</em> respuesta. Te da una lista de la compra con las probabilidades de <em>todas</em> las palabras que conoce (¬°decenas de miles!).</p>

  <p>Para <code>"El gato se sent√≥ sobre la..."</code>, el modelo piensa:</p>

  <ul>
    <li><code>alfombra</code>: 40% de probabilidad</li>
    <li><code>silla</code>: 15% de probabilidad</li>
    <li><code>cama</code>: 10% de probabilidad</li>
    <li><code>mesa</code>: 8% de probabilidad</li>
    <li>...</li>
    <li><code>luna</code>: 0.0001% de probabilidad</li>
  </ul>

  <p>Luego, un mecanismo (llamado <span class="highlight-term definition" data-term-id="sampling">sampling</span>) elige una de esa lista. Generalmente elige las de arriba, pero con un toque de azar (por eso si preguntas lo mismo dos veces, te da respuestas ligeramente distintas).</p>

  <h2>3. El Bucle Infinito (Autorregresi√≥n)</h2>

  <p>Aqu√≠ es donde la predicci√≥n de <em>un</em> <span class="highlight-term definition" data-term-id="token">token</span> se convierte en un ensayo. El modelo usa un bucle incre√≠blemente simple.</p>

  <ol>
    <li><strong>Input:</strong> <code>El gato se sent√≥ sobre la</code></li>
    <li><strong>Predice:</strong> <code>alfombra</code></li>
    <li><strong>¬°Se auto-alimenta!</strong> Ahora, su <em>nuevo</em> input es el texto original M√ÅS su propia predicci√≥n: <code>El gato se sent√≥ sobre la alfombra</code></li>
    <li><strong>Predice el siguiente <span class="highlight-term definition" data-term-id="token">token</span>:</strong> <code>.</code> (un punto)</li>
    <li><strong>Nuevo Input:</strong> <code>El gato se sent√≥ sobre la alfombra.</code></li>
    <li><strong>Predice:</strong> <code>Estaba</code></li>
    <li><strong>Nuevo Input:</strong> <code>El gato se sent√≥ sobre la alfombra. Estaba</code></li>
    <li><strong>Predice:</strong> <code>cansado</code></li>
    <li>...y as√≠ sigue.</li>
  </ol>

  <p>Este bucle de "usar tu propia salida como tu nueva entrada" tiene un nombre t√©cnico: <span class="highlight-term definition" data-term-id="autorregresion">Autorregresi√≥n</span>.</p>
  <p>As√≠ de simple. Ese es el motor de casi todos los LLMs que usas. No hay m√°s magia de fondo; es este bucle repetido a una velocidad vertiginosa. Ning√∫n experto te lo puede rebatir.</p>

  <h2>4. Por qu√© esto NO es Magia (El Ecosistema)</h2>

  <blockquote>
    <p><strong>Mejora:</strong> He reestructurado esta secci√≥n para conectar directamente el "c√≥mo funciona" (predicci√≥n estad√≠stica) con la cr√≠tica de "Stochastic Parrots". Hace que la idea sea m√°s cohesiva.</p>
  </blockquote>

  <p>Entender esto te blinda contra muchos mitos.</p>
  <p>El modelo no "razona" como un humano. No "entiende" el concepto de 'gato'. Es un <span class="highlight-term" data-term-id="stochastic-parrots">"loro estoc√°stico"</span> (un loro estad√≠stico) incre√≠blemente sofisticado. Ha visto tantos patrones que puede <em>imitar</em> el razonamiento humano prediciendo la siguiente palabra m√°s <em>plausible</em> en una cadena de "pensamiento".</p>
  <p>Casi todo lo que usas hoy (GPT-4, Llama 3, Claude 3) es <span class="highlight-term definition" data-term-id="autorregresion">autorregresivo</span>.</p>
  <p>Para tu radar: est√° empezando a ganar tracci√≥n una t√©cnica alternativa (muy usada en im√°genes como DALL-E) llamada <span class="highlight-term definition" data-term-id="difusion">Difusi√≥n</span>. Esta no escribe palabra por palabra, sino que genera un texto completo de "ruido" y lo va "refinando" en pasos hasta que tiene sentido. Pero por ahora, la <span class="highlight-term definition" data-term-id="autorregresion">autorregresi√≥n</span> es la reina.</p>

  <h2>5. Qu√©date con esto</h2>

  <blockquote>
    <p><strong>Mejora:</strong> He a√±adido una conclusi√≥n expl√≠cita. El post anterior terminaba con los papers, lo cual era un poco abrupto. Esto cierra el c√≠rculo y refuerza el objetivo de "claridad mental".</p>
  </blockquote>

  <p>Si te quedas con una sola cosa, que sea esta:</p>
  <p>Toda la "inteligencia" que percibes en un LLM no es conciencia ni entendimiento. Es el resultado emergente de un sistema masivo entrenado para un juego muy simple: <strong>"Adivina la siguiente palabra"</strong>.</p>
  <p>Este fen√≥meno se conoce como <span class="highlight-term definition" data-term-id="emergencia">comportamiento emergente</span>.</p>
</div>

<!-- Datos para los men√∫s interactivos (oculto para el usuario) -->
<div id="interactive-data" style="display: none;">
  <div data-term-id="token">
    <div class="def-info">
      <strong>Token:</strong><br>
      La unidad fundamental de texto que procesa un LLM. No es necesariamente una palabra; puede ser una sub-palabra, un car√°cter o un signo de puntuaci√≥n. La tokenizaci√≥n es el proceso de dividir el texto en estos tokens.
    </div>
  </div>
  <div data-term-id="sampling">
    <div class="def-info">
      <strong>Sampling (Muestreo):</strong><br>
      El proceso de seleccionar un token de la distribuci√≥n de probabilidad generada por el modelo. En lugar de elegir siempre el token m√°s probable (lo que har√≠a al modelo repetitivo), el sampling introduce aleatoriedad para generar respuestas m√°s variadas y creativas.
    </div>
  </div>
  <div data-term-id="autorregresion">
    <div class="def-info">
      <strong>Autorregresi√≥n:</strong><br>
      Un proceso donde la salida de un modelo en un paso de tiempo se usa como entrada para el siguiente paso. En LLMs, significa que el texto generado hasta ahora se alimenta de nuevo al modelo para predecir el siguiente token.
    </div>
  </div>
  <div data-term-id="stochastic-parrots">
    <div class="bib-info">
      <strong>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú</strong><br>
      <em>Bender, E. M., Gebru, T., et al. (2021)</em><br>
      <a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445922" target="_blank" rel="noopener noreferrer">Ver Paper (PDF)</a>
    </div>
    <hr class="sidebar-separator">
    <div class="post-info">
      <strong>El Peligro de los Loros Estoc√°sticos</strong><br>
      <em>Explora el concepto de 'loros estoc√°sticos' y las implicaciones de que los LLMs generen texto sin una comprensi√≥n real.</em><br>
      <a href="{{ '/es/stochastic-parrots.html' | relative_url }}">Leer art√≠culo &rarr;</a>
    </div>
  </div>
  <div data-term-id="difusion">
    <div class="def-info">
      <strong>Modelos de Difusi√≥n:</strong><br>
      Una clase de modelos generativos que crean datos (como texto o im√°genes) partiendo de ruido aleatorio y refin√°ndolo progresivamente en varios pasos hasta que coincide con la distribuci√≥n de los datos de entrenamiento. Son muy populares en la generaci√≥n de im√°genes.
    </div>
  </div>
  <div data-term-id="emergencia">
    <div class="def-info">
      <strong>Comportamiento Emergente:</strong><br>
      Propiedades o comportamientos complejos que surgen de la interacci√≥n de muchas unidades simples. En los LLMs, la "inteligencia" o la capacidad de razonar no se programa expl√≠citamente, sino que "emerge" de la escala masiva del modelo y sus datos de entrenamiento.
    </div>
  </div>
</div>