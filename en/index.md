---
layout: default
---
# ðŸ“š Foundational Papers in Modern AI

A curated collection of the most influential papers that have defined the landscape of modern Artificial Intelligence. Ideal for a quick reference to the theoretical pillars of Deep Learning.

---

## ðŸ¤– Transformer Architectures and Language Models (LLMs)

### Attention Is All You Need
- **Authors:** Ashish Vaswani, et al.
- **Year:** 2017
- **Publication:** [arXiv:1706.03762](https://arxiv.org/pdf/1706.03762)
- **Quick Summary:** Introduces the **Transformer** architecture, based solely on attention mechanisms. It eliminated the need for recurrence (RNNs) and allowed for massive parallelization, laying the groundwork for most modern language models like GPT and BERT.

---

### Language Models are Few-Shot Learners (GPT-3)
- **Authors:** Tom B. Brown, et al.
- **Year:** 2020
- **Publication:** [arXiv:2005.14165](https://arxiv.org/pdf/2005.14165)
- **Quick Summary:** Took LLMs to an unprecedented scale (175 billion parameters). It introduced the concept of "in-context learning" or *few-shot* learning, where the model can learn to perform a task just by seeing a few examples in the prompt.
