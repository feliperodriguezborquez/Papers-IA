<!DOCTYPE html>
<html lang="es" data-theme="dark">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Attention Is All You Need</title>

  <link rel="stylesheet" href="/assets/css/main.css">

</head>
<body>
  
  

  
  
  
  

  <header class="site-header-main">
    <div class="header-content">
      <a href="/" class="site-title-link"><h1>IA en 5 min</h1></a>
      <button id="theme-toggle" title="Cambiar tema">ðŸŒ™</button>
    </div>
  </header>

  <!-- MenÃºs laterales ocultos -->
  <div id="bibliography-sidebar" class="sidebar left-sidebar">
    <div class="sidebar-header">
      <h3>BibliografÃ­a</h3>
      <button class="close-sidebar-btn" title="Cerrar">&times;</button>
    </div>
    <div class="sidebar-content"></div>
  </div>
  <div id="definitions-sidebar" class="sidebar right-sidebar">
    <div class="sidebar-header">
      <h3>Definiciones</h3>
      <button class="close-sidebar-btn" title="Cerrar">&times;</button>
    </div>
    <div class="sidebar-content"></div>
  </div>

  <div class="container">
    

  <div class="paper-detail">
  <div class="post-header">
    <a href="./#attention-is-all-you-need">&larr; Volver a la lista</a>
    
  </div>

  <h1>Attention Is All You Need (2017)</h1>

  <p class="paper-meta">
    <strong>Authors:</strong> Ashish Vaswani, et al.<br />
    <strong>Publication:</strong> <a href="https://arxiv.org/pdf/1706.03762" target="_blank" rel="noopener noreferrer">arXiv:1706.03762</a>
  </p>

  <hr />

  <h2>Extended Summary</h2>
  <p>
    The paper "Attention Is All You Need" is one of the most transformative works in the field of Natural Language Processing (NLP). Its main contribution was the introduction of the **Transformer** architecture, which completely departed from the recurrent (RNN) and convolutional (CNN) architectures that had dominated the field until then.
  </p>
  <p>
    The core idea is that to model dependencies in sequences, it is not necessary to process the data in order. Instead, the model can "pay attention" to any part of the input sequence when generating a part of the output. This is achieved through the **attention mechanism**, specifically "Scaled Dot-Product Attention" and "Multi-Head Attention".
  </p>

  <h2>Key Points</h2>
  <ul>
    <li><strong>Elimination of Recurrence:</strong> By not depending on the hidden state of the previous step, the Transformer can process entire sequences in parallel, resulting in much faster training.</li>
    <li><strong>Multi-Head Attention:</strong> Allows the model to focus on different parts of the sequence and capture different types of relationships (syntactic, semantic, etc.) simultaneously.</li>
    <li><strong>Positional Encodings:</strong> Since the model has no inherent notion of word order, "positional encodings" are injected into the input embeddings to give them information about their position in the sequence.</li>
  </ul>

  <h2>Impact on AI</h2>
  <p>
    The impact of the Transformer has been monumental. It is the fundamental architecture behind almost all modern large-scale language models (LLMs), including the GPT family of models (from OpenAI), BERT (from Google), and many others. Its efficiency and scalability enabled the training of models with billions of parameters, leading to the generative AI revolution we see today.
  </p>

</div>


  
  </div>

  <script src="/assets/js/theme.js"></script>
  <script src="/assets/js/interactive.js"></script>
</body>
</html>