<!DOCTYPE html>
<html lang="es" data-theme="dark">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>游닄 Papers Fundamentales de la IA Moderna</title>

  <link rel="stylesheet" href="/assets/css/main.css">

</head>
<body>
  
  

  
  
  
  

  <header class="site-header-main">
    <div class="header-content">
      <a href="/" class="site-title-link"><h1>IA en 5 min</h1></a>
      <button id="theme-toggle" title="Cambiar tema">游깿</button>
    </div>
  </header>

  <!-- Men칰s laterales ocultos -->
  <div id="bibliography-sidebar" class="sidebar left-sidebar">
    <div class="sidebar-header">
      <h3>Bibliograf칤a</h3>
      <button class="close-sidebar-btn" title="Cerrar">&times;</button>
    </div>
    <div class="sidebar-content"></div>
  </div>
  <div id="definitions-sidebar" class="sidebar right-sidebar">
    <div class="sidebar-header">
      <h3>Definiciones</h3>
      <button class="close-sidebar-btn" title="Cerrar">&times;</button>
    </div>
    <div class="sidebar-content"></div>
  </div>

  <div class="container">
    
      <section class="search-section">
        <input type="text" id="paper-search" placeholder="Buscar papers..." aria-label="Buscar papers">
      </section>
    

  <h1 id="-papers-fundamentales-de-la-ia-moderna">游닄 Papers Fundamentales de la IA Moderna</h1>

<p>Una colecci칩n curada de los papers m치s influyentes que han definido el panorama de la Inteligencia Artificial moderna. Ideal para tener una referencia r치pida de los pilares te칩ricos del Deep Learning.</p>

<hr />

<div class="papers-list">

  <section class="paper-category">
    <h2>游뱄 Arquitecturas Transformer y Modelos de Lenguaje (LLMs)</h2>

    <div class="paper-item" id="attention-is-all-you-need" data-category="LLMs, Transformers">
      <h3><a href="papers/attention-is-all-you-need.html">Attention Is All You Need</a></h3>
      <p class="paper-meta">
        <strong>Autores:</strong> Ashish Vaswani, et al.<br />
        <strong>A침o:</strong> 2017<br />
        <strong>Publicaci칩n:</strong> <a href="https://arxiv.org/pdf/1706.03762">arXiv:1706.03762</a>
      </p>
      <p class="paper-summary">Introdujo la arquitectura **Transformer** (encoder-decoder), ocupando como 칰nica base el mecanismo de atenci칩n, prescindiendo as칤 de las antiguas RNNs. Adem치s, realiz칩 toda la ingenier칤a necesaria para que el modelo sea funcional. Sent칩 las bases para los modernos LLMs.</p>
    </div>

    <div class="paper-item" id="gpt3" data-category="LLMs, Few-Shot Learning">
      <h3>Language Models are Few-Shot Learners (GPT-3)</h3>
      <p class="paper-meta">
        <strong>Autores:</strong> Tom B. Brown, et al.<br />
        <strong>A침o:</strong> 2020<br />
        <strong>Publicaci칩n:</strong> <a href="https://arxiv.org/pdf/2005.14165">arXiv:2005.14165</a>
      </p>
      <p class="paper-summary">Llev칩 los LLMs a una escala sin precedentes (175 mil millones de par치metros). Introdujo el concepto de "in-context learning" o aprendizaje *few-shot*, donde el modelo puede aprender a realizar una tarea con solo ver unos pocos ejemplos en el prompt.</p>
    </div>

    <div class="paper-item" id="next-token-prediction" data-category="LLMs, Conceptos Fundamentales">
      <h3><a href="next-token-prediction.html">Predicci칩n del Siguiente Token</a></h3>
      <p class="paper-meta">
        <strong>Tipo:</strong> Art칤culo Explicativo<br />
        <strong>Concepto Clave:</strong> Generaci칩n Autorregresiva
      </p>
      <p class="paper-summary">Explica de forma sencilla el motor de todos los LLMs modernos: el bucle de predecir la siguiente palabra (o token), a침adirla al texto, y repetir. La base de la IA generativa actual.</p>
    </div>

  </section>

</div>


  
  </div>

  <script src="/assets/js/theme.js"></script>
  <script src="/assets/js/interactive.js"></script>
</body>
</html>