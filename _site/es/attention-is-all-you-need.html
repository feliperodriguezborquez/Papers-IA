<!DOCTYPE html>
<html lang="es" data-theme="dark">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Attention Is All You Need</title>

  <link rel="stylesheet" href="/assets/css/main.css">

</head>
<body>
  
  

  
  
  
  

  <header class="site-header-main">
    <div class="header-content">
      <a href="/" class="site-title-link"><h1>IA en 5 min</h1></a>
      <button id="theme-toggle" title="Cambiar tema"></button>
    </div>
  </header>

  <!-- Men煤s laterales ocultos -->
  <div id="bibliography-sidebar" class="sidebar left-sidebar">
    <div class="sidebar-header">
      <h3>Bibliograf铆a</h3>
      <button class="close-sidebar-btn" title="Cerrar">&times;</button>
    </div>
    <div class="sidebar-content"></div>
  </div>
  <div id="definitions-sidebar" class="sidebar right-sidebar">
    <div class="sidebar-header">
      <h3>Definiciones</h3>
      <button class="close-sidebar-btn" title="Cerrar">&times;</button>
    </div>
    <div class="sidebar-content"></div>
  </div>

  <div class="container">
    

  <div class="paper-detail">
  <div class="post-header">
    <a href="./#attention-is-all-you-need">&larr; Volver a la lista</a>
    
  </div>

  <h1>Attention Is All You Need (2017)</h1>

  <p class="paper-meta">
    <strong>Autores:</strong> Ashish Vaswani, et al.<br />
    <strong>Publicaci贸n:</strong> <a href="https://arxiv.org/pdf/1706.03762" target="_blank" rel="noopener noreferrer">arXiv:1706.03762</a>
  </p>

  <hr />

  <h2>Resumen Extendido</h2>
  <p>
    El paper "Attention Is All You Need" es uno de los trabajos m谩s transformadores en el campo del Procesamiento del Lenguaje Natural (PLN). Su contribuci贸n principal fue la introducci贸n de la arquitectura **Transformer**, que se desvi贸 por completo de las arquitecturas recurrentes (RNN) y convolucionales (CNN) que dominaban el campo hasta ese momento.
  </p>
  <p>
    La idea central es que, para modelar dependencias en secuencias, no es necesario procesar los datos en orden. En su lugar, el modelo puede "prestar atenci贸n" a cualquier parte de la secuencia de entrada cuando genera una parte de la salida. Esto se logra a trav茅s del **mecanismo de atenci贸n**, espec铆ficamente "Scaled Dot-Product Attention" y "Multi-Head Attention".
  </p>

  <h2>Puntos Clave</h2>
  <ul>
    <li><strong>Eliminaci贸n de la Recurrencia:</strong> Al no depender del estado oculto del paso anterior, el Transformer puede procesar secuencias enteras en paralelo, lo que resulta en entrenamientos mucho m谩s r谩pidos.</li>
    <li><strong>Multi-Head Attention:</strong> Permite al modelo enfocarse en diferentes partes de la secuencia y capturar distintos tipos de relaciones (sint谩cticas, sem谩nticas, etc.) simult谩neamente.</li>
    <li><strong>Positional Encodings:</strong> Como el modelo no tiene una noci贸n inherente del orden de las palabras, se inyectan "codificaciones posicionales" a los embeddings de entrada para darles informaci贸n sobre su posici贸n en la secuencia.</li>
  </ul>

  <h2>Impacto en la IA</h2>
  <p>
    El impacto del Transformer ha sido monumental. Es la arquitectura fundamental detr谩s de casi todos los modelos de lenguaje a gran escala (LLMs) modernos, incluyendo la familia de modelos GPT (de OpenAI), BERT (de Google), y muchos otros. Su eficiencia y escalabilidad permitieron el entrenamiento de modelos con miles de millones de par谩metros, dando lugar a la revoluci贸n de la IA generativa que vemos hoy.
  </p>

</div>


  
  </div>

  <script src="/assets/js/theme.js"></script>
  <script src="/assets/js/interactive.js"></script>
</body>
</html>